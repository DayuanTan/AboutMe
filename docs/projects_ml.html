<head>
  <title>Dayuan Tan's Home Page</title>
  <meta charset="utf-8">
  <meta name="google-site-verification" content="JNhDbMaka9oOIattEtQC5-1CL_uOnJ_KoJ7wT67A9NA" />
  <meta name="Dayuan Tan" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

  <link rel="icon" href="img/d.ico" >
  
  <script src="components/header.js" type="text/javascript" defer></script>
  <script src="components/footer.js" type="text/javascript" defer></script>
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-WZKRRN0DSY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-WZKRRN0DSY');
  </script>

  <link rel="stylesheet" href="index.css">
  <script src="./components/open_links_in_new_tab.js"></script>

</head>


<body>

<header-component></header-component>  
<main>


<div class="container">
  <div class="row">




    <div class="col-sm-12"> 
      



      <a id="machinelearning">
        <h1 style="padding-top: 40px; margin-top: -40px;"></h1>
      </a>
      <br/>
      <br/>
      
      <h5><a  href="https://github.com/DayuanTan/xxx">Machine Learning</a></h5>
      <p><strong>Key Words:</strong> Machine Learning, PyTorch</p>
      <p><strong>Tech Stack:</strong> Python, PyTorch </p>
      
      <p>Machine Learning Examples:</p>
      <ol>
        <li><strong>Linear Regression Example</strong> 
          <ul>
            <li><strong>Key Words: </strong>
              <ul>
                <li><strong>Linear Regression</strong></li> 
                <li><strong>Mini-batch Stochastic Gradient Descent (SGD)</strong> - Optimization Algorithm </li> 
                <li><strong>Mean Square Error (MSE)</strong> - Loss Function </li> 
              </ul>            
            </li>
            
            <li>A complete process: 
              <ul>
                <li>Read Dataset -> Define Model -> Initialize Parameter -> Define Loss -> Optimization Algorithm -> Train. 
                </li> 
                <li>数据读取 -> 模型定义 -> 模型参数初始化 -> 损失函数 -> 优化算法 -> 训练模块.
                </li>
              </ul>
            </li>    
            
            <li>Mini-batch Stochastic Gradient Descent (SGD) is implemented.
              <ul>
                <li>Implement from scratch. <a href="./ml/Dayuan_Linear_Regression_Scratch.html">HTML</a> (Online View), <a href="./ml/Dayuan_Linear_Regression_Scratch.ipynb">Jupyter Notebook</a> (Download).
                </li>  
                <li>Concise Implementation - Use PyTorch. <a href="./ml/Dayuan_Linear_Regression_Concise.html">HTML</a>, <a href="./ml/Dayuan_Linear_Regression_Concise.ipynb">Jupyter Notebook</a> (Download).
                </li>  
              </ul>
            </li>          
          </ul>
        </li>

        <li><strong>Softmax Regression (Actual Classification) Example</strong>  
          <ul>
            <li><strong>Key Words: </strong>
              <ul>
                <li><strong>Softmax Regression (Classification), </strong> </li>
                <li>L1 Loss, L2 Loss (MSE), Huber's Robust Loss, <strong>Cross-Entropy Loss</strong>, </li>
                <li>MNIST, ImageNet</li>
              </ul>
            </li>
            
            <li>Implement from scratch. <a href="./ml/Dayuan-softmax-regression-scratch.html">HTML</a>, <a href="./ml/Dayuan-softmax-regression-scratch.ipynb">Jupyter Notebook</a> (Download).
            </li>  
            <li>Concise Implementation - Use PyTorch. <a href="./ml/Dayuan-softmax-regression-concise.html">HTML</a>, <a href="./ml/Dayuan-softmax-regression-concise.ipynb">Jupyter Notebook</a> (Download).
            </li>  

            <li>Notes:
              <ul>
                
                <li>Softmax function can transform unnormalized predictions into non-negative numbers that sum to 1, while keeping the model differentiable.
                </li>
                <ul> 
                  <li>softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。                
                  </li>
                </ul>
                
                <li>Although softmax is a non-linear function, the output of softmax regression is still determined by an affine transformation of input features. Therefore, softmax regression is a linear model.
                </li>
                <ul>
                  <li>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）.                  
                  </li>
                </ul>
              </ul>
            </li>

                
          </ul>
        </li>

        <li><strong>Perceptrons & Multilayer Perceptrons Example</strong>  
          <ul>
            <li><strong>Key Words: </strong>
              <ul>
                <li><strong>Perceptrons</strong>  (Binary Classification)</li>
                <li><strong>Multilayer Perceptrons</strong>  (Multiple Classification)</li>
                <li><strong>Activation Function </strong>(Introduce non-linearity): </li>
                <ul>
                  <li>Sigmoid, Tanh, RelU. Softmax (as output layer of a NN for multi-class classification).                    
                  </li>
                  <a href="img_ml/activation-sigmoid.svg" target="_blank">
                    <img src="img_ml/activation-sigmoid.svg" title="Activation Function Sigmoid" alt="Activation Function Sigmoid" style="width:100px">
                  </a>
                  <a href="img_ml/activation-sigmoid-gradient.svg" target="_blank">
                    <img src="img_ml/activation-sigmoid-gradient.svg" title="Activation Function Sigmoid Gradient" alt="Activation Function Sigmoid Gradient" style="width:100px">
                  </a>
                  <a href="img_ml/activation-tanh.svg" target="_blank">
                    <img src="img_ml/activation-tanh.svg" title="Activation Function tanh" alt="Activation Function tanh" style="width:100px">
                  </a>
                  <a href="img_ml/activation-tanh-gradient.svg" target="_blank">
                    <img src="img_ml/activation-tanh-gradient.svg" title="Activation Function tanh Gradient" alt="Activation Function tanh Gradient" style="width:100px">
                  </a>
                  <a href="img_ml/activation-relu.svg" target="_blank">
                    <img src="img_ml/activation-relu.svg" title="Activation Function Relu" alt="Activation Function Relu" style="width:100px">
                  </a>
                  <a href="img_ml/activation-relu-gradient.svg" target="_blank">
                    <img src="img_ml/activation-relu-gradient.svg" title="Activation Function Relu Gradient" alt="Activation Function Relu Gradient" style="width:100px">
                  </a>
                </ul>
              </ul>
            </li>
            
            <li>Implement from scratch. <a href="./ml/Dayuan-softmax-regression-scratch.html">HTML</a>, <a href="./ml/Dayuan-softmax-regression-scratch.ipynb">Jupyter Notebook</a> (Download).
            </li>  
            <li>Concise Implementation - Use PyTorch. <a href="./ml/Dayuan-softmax-regression-concise.html">HTML</a>, <a href="./ml/Dayuan-softmax-regression-concise.ipynb">Jupyter Notebook</a> (Download).
            </li>  

            <li>Notes:
              <ul>
                <li>Why is MLP used instead of SVM? Because if the performance of MLP is not good, it can be easily transformed into DNN, Transformer, etc. However, using SVM would require many changes. There are also not many aspects of SVM that can be adjusted. Additionally, SVM is not suitable for large-scale data in the millions.
                </li>
                <ul>
                  <li>为什么用MLP没用SVM。因为MLP如果效果不好以后可转DNN、transformcer等很方便。但是用SVM要改很多。SVM能调的东西也不多。SVM也不适合大百万级数据。
                  </li>
                </ul>
              </ul>
            </li>

          </ul>
        </li>




        <li><strong>Transformer Example</strong>  
          <ul>
            <li><strong>Key Words: </strong>
              <ul>
                <li><strong>Sxxx </strong> </li>
                <li>L1 Loss, L2 Loss (MSE), Huber's Robust Loss, <strong>Cross-Entropy Loss</strong>, </li>
                <li>MNIST, ImageNet</li>
              </ul>
            </li>
            
            <li>Implement from scratch. <a href="./ml/Dayuan-softmax-regression-scratch.html">HTML</a>, <a href="./ml/Dayuan-softmax-regression-scratch.ipynb">Jupyter Notebook</a> (Download).
            </li>  
            <li>Concise Implementation - Use PyTorch. <a href="./ml/Dayuan-softmax-regression-concise.html">HTML</a>, <a href="./ml/Dayuan-softmax-regression-concise.ipynb">Jupyter Notebook</a> (Download).
            </li>  

            <li>Notes:
              <ul>
                
                <li>Softmax function can transform unnormalized predictions into non-negative numbers that sum to 1, while keeping the model differentiable.
                </li>
                <ul> 
                  <li>softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。                
                  </li>
                </ul>
                
                <li>Although softmax is a non-linear function, the output of softmax regression is still determined by an affine transformation of input features. Therefore, softmax regression is a linear model.
                </li>
                <ul>
                  <li>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）.                  
                  </li>
                </ul>
              </ul>
            </li>

                
          </ul>
        </li>





      </ol>
     





      
      <br/>
    </div>







  </div>
</div>

</main>
<footer-component></footer-component>


</body>
</html>

