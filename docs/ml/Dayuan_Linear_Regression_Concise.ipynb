{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a7d68b-3483-42d5-9ae1-36df7438be9f",
   "metadata": {},
   "source": [
    "# Linear Regression Example - Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674e2ed-2f22-4d35-aa16-71cfd5b63dbd",
   "metadata": {},
   "source": [
    "## 生成数据集 Generate fake data as training data (samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c022b3b3-8a0e-4579-a989-75e789af5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b589e5-9620-4121-88e5-d0dec8bfe4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some fake data\n",
    "\n",
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    \"\"\"Generate y=Xw+b and noises for it\"\"\"\n",
    "    # X is a tensor a random numbers which are drawn from a normal distributions whose mean is 0 and standard deviation is 1. \n",
    "    # It has num_examples rows, len(w) columns.\n",
    "    X = torch.normal(0, 1, (num_examples, len(w))) #正态分布 均值为0 方差为1 的随机数。 行数：num_examples。 列数: len(w)\n",
    "    y = torch.matmul(X, w) + b # y = Xw + b\n",
    "    # Add some noises for y. The shape of the noises is as same as y's. \n",
    "    y += torch.normal(0, 0.01, y.shape) # 加点噪音，噪音形状和y的形状相同 \n",
    "    \n",
    "    # return y as a column vector\n",
    "    return X, y.reshape((-1, 1)) # y变成列向量返回\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8295e14-5763-43d3-8823-04ec6df6eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3cb75c-80c0-49e6-9921-a4a1bf7d4685",
   "metadata": {},
   "source": [
    "## 读取数据集 Read dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fc19e-5dd5-47f6-bc0a-d20e19cda0b8",
   "metadata": {},
   "source": [
    "我们可以[调用框架中现有的API来读取数据]。 我们将features和labels作为API的参数传递，并通过数据迭代器指定batch_size。 此外，布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。\n",
    "\n",
    "We can [use the existing APIs in the framework to read data]. We pass features and labels as parameters to the API and specify the batch_size through the data iterator. Moreover, the boolean value is_train indicates whether we want the data iterator object to shuffle the data in each iteration cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84268a11-267b-4499-b729-99158ff9e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays) # features和labels组成list作为data_arrays传给TensorDataset(), which把把它转成PyTorch的tensor\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train) # DataLoader从dataset中每次以shuffle的方式（即随机的方式）取batch_size大小的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2892a1b-a941-4492-9ae5-a6773b71e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f4306-3f23-48ec-bb1b-8364ee1730f0",
   "metadata": {},
   "source": [
    "使用data_iter的方式与我们在scratch中使用data_iter函数的方式相同。为了验证是否正常工作，让我们读取并打印第一个小批量样本。 \n",
    "\n",
    "与scratch不同，这里我们使用iter构造Python迭代器，并使用next从迭代器中获取第一项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4845b3a3-47a0-4647-b254-70d8767a0ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1407,  0.1276],\n",
       "         [ 0.8594, -0.0988],\n",
       "         [-1.3422,  0.0502],\n",
       "         [ 2.6259,  1.1320],\n",
       "         [ 1.8280, -0.2564],\n",
       "         [-0.9355,  0.8142],\n",
       "         [ 0.9108, -0.0376],\n",
       "         [-0.9746,  0.3416],\n",
       "         [ 0.2967,  1.0541],\n",
       "         [ 0.0256,  0.5819]]),\n",
       " tensor([[ 3.4846],\n",
       "         [ 6.2583],\n",
       "         [ 1.3561],\n",
       "         [ 5.6188],\n",
       "         [ 8.7156],\n",
       "         [-0.4448],\n",
       "         [ 6.1490],\n",
       "         [ 1.1163],\n",
       "         [ 1.2060],\n",
       "         [ 2.2765]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466c598-e242-4ec6-9e1c-8758e1434b90",
   "metadata": {},
   "source": [
    "## Define the model 定义模型\n",
    "\n",
    "当我们在scratch中实现线性回归时，\n",
    "我们明确定义了模型参数变量，并编写了计算的代码，这样通过基本的线性代数运算得到输出。\n",
    "但是，如果模型变得更加复杂，且当我们几乎每天都需要实现模型时，自然会想简化这个过程。\n",
    "\n",
    "\n",
    "对于标准深度学习模型，我们可以[**使用框架的预定义好的层**]。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。\n",
    "\n",
    "Linear Regression 可以被理解成只有一层的神经网络。\n",
    "\n",
    "我们首先定义一个模型变量`net`，它是一个`Sequential`类的实例。\n",
    "`Sequential`类将多个层串联在一起。**Sequential就是一个list of layers.**\n",
    "当给定输入数据时，`Sequential`实例将数据传入到第一层，\n",
    "然后将第一层的输出作为第二层的输入，以此类推。\n",
    "在下面的例子中，我们的模型只包含一个层，因此实际上不需要`Sequential`。\n",
    "但是由于以后几乎所有的模型都是多层的，在这里使用`Sequential`会让你熟悉“标准的流水线”。\n",
    "\n",
    "回顾single_neuron中的单层网络架构，\n",
    "这一单层被称为*全连接层*（fully-connected layer），\n",
    "因为它的每一个输入都通过矩阵-向量乘法得到它的每个输出。\n",
    "\n",
    "\n",
    "When we implemented linear regression from scratch, we explicitly defined the model parameter variables and wrote the code to compute the output using basic linear algebra operations. However, if the model becomes more complex, and we find ourselves needing to implement models almost daily, it's natural to want to simplify this process.\n",
    "\n",
    "For standard deep learning models, we can [use the predefined layers from the framework]. This allows us to focus on which layers to use to construct the model, rather than the implementation details of the layers.\n",
    "\n",
    "Linear Regression can be treated as a one-layer neural network.\n",
    "\n",
    "We first define a model variable net, which is an instance of the Sequential class. The Sequential class chains multiple layers together. When given input data, a Sequential instance passes the data to the first layer, then takes the output of the first layer as the input to the second layer, and so on. In the example below, our model only contains one layer, so in reality, we don't need Sequential. However, since almost all models later on are multi-layered, using Sequential here will get you familiar with the 'standard pipeline'.\n",
    "\n",
    "Recall the single-layer network architecture in single_neuron. This single layer is known as a fully-connected layer (fully-connected layer), because each of its inputs is connected to each of its outputs through matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c811f8-4f1c-448c-8797-94b574d36748",
   "metadata": {},
   "source": [
    "在PyTorch中，全连接层在Linear类中定义。 值得注意的是，我们将两个参数传递到nn.Linear中。 第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。\n",
    "\n",
    "In PyTorch, the fully connected layer is defined in the Linear class. It's worth noting that we pass two parameters to nn.Linear. The first specifies the shape of the input features, which is 2, and the second specifies the shape of the output features. The output feature shape is a single scalar, therefore it is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4210cbbe-fb9d-46ee-8ce6-1f67a46bdb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn是神经网络的缩写\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8205779-b19a-418f-881f-dc96230a77da",
   "metadata": {},
   "source": [
    "## Initializing model's parameters (**初始化模型参数**)\n",
    "\n",
    "在使用`net`之前，我们需要初始化模型参数。\n",
    "如在线性回归模型中的权重和偏置。\n",
    "深度学习框架通常有预定义的方法来初始化参数。\n",
    "在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样，\n",
    "偏置参数将初始化为零。\n",
    "\n",
    "正如我们在构造`nn.Linear`时指定输入和输出尺寸一样，\n",
    "现在我们能直接访问参数以设定它们的初始值。\n",
    "我们通过`net[0]`选择网络中的第一个图层，\n",
    "然后使用`weight.data`和`bias.data`方法访问参数。\n",
    "我们还可以使用替换方法`normal_`和`fill_`来重写参数值。\n",
    "\n",
    "Before we use net, we need to initialize the model parameters, such as the weights and biases in the linear regression model. Deep learning frameworks typically have predefined methods for initializing parameters. Here, we specify that each weight parameter should be randomly sampled from a normal distribution with a mean of 0 and a standard deviation of 0.01, and the bias parameters will be initialized to zero.\n",
    "\n",
    "Just as we specified input and output dimensions when constructing nn.Linear, now we can directly access the parameters to set their initial values. We select the first layer in the network with net[0], and then access the parameters using the weight.data and bias.data methods. We can also use the replacement methods normal_ and fill_ to overwrite parameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af01da53-dd9f-4d7d-8eeb-3cd993ba8c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0, 0.01) # 这里等同于我们手动实现初始化模型参数w\n",
    "net[0].bias.data.fill_(0) # 这里等同于我们手动实现初始化模型参数b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f195b95-5a05-49b5-b13d-b2517fb10037",
   "metadata": {},
   "source": [
    "## Define the loss function 定义损失函数\n",
    "\n",
    "[**计算均方误差使用的是`MSELoss`类，也称为平方$L_2$范数**]。\n",
    "默认情况下，它返回所有样本损失的平均值。\n",
    "\n",
    "[For computing the mean squared error, the MSELoss class is used, which is also known as the squared $L_2$ norm]. By default, it returns the average of the losses over all the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a170aa14-440e-4245-a663-6fb8c37b216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e853929-b474-45c2-8f84-9946c668ce04",
   "metadata": {},
   "source": [
    "## Define the Optimization Algorithm 定义优化算法\n",
    "\n",
    "小批量随机梯度下降算法是一种优化神经网络的标准工具，\n",
    "PyTorch在`optim`模块中实现了该算法的许多变种。\n",
    "\n",
    "\n",
    "当我们(**实例化一个`SGD`实例**)时，我们要指定优化的参数\n",
    "（可通过`net.parameters()`从我们的模型中获得）以及优化算法所需的超参数字典。\n",
    "小批量随机梯度下降只需要设置`lr`值，这里设置为0.03。\n",
    "\n",
    "Mini-batch stochastic gradient descent is a standard tool for optimizing neural networks, and PyTorch implements many variants of this algorithm in the optim module. \n",
    "\n",
    "When we (instantiate an SGD instance), we need to specify the parameters to optimize (which can be obtained from our model via net.parameters()) and a dictionary of hyperparameters required by the optimization algorithm. Mini-batch stochastic gradient descent only needs the lr value to be set, which we set here to 0.03.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1362683c-8f39-440b-becf-0e74b2956943",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013e7a4-4644-40cf-a72a-176491fe8cfc",
   "metadata": {},
   "source": [
    "## Training 训练\n",
    "\n",
    "通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。\n",
    "我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。\n",
    "当我们需要更复杂的模型时，高级API的优势将大大增加。\n",
    "\n",
    "\n",
    "当我们有了所有的基本组件，[**训练过程代码与我们从零开始实现时所做的非常相似**]。\n",
    "\n",
    "\n",
    "回顾一下：在每个迭代周期里，我们将完整遍历一次数据集（`train_data`），\n",
    "\n",
    "不停地从中获取一个小批量的输入和相应的标签。\n",
    "\n",
    "对于每一个小批量，我们会进行以下步骤:\n",
    "\n",
    "* 通过调用`net(X)`生成预测并计算损失`l`（前向传播）。\n",
    "* 通过进行反向传播来计算梯度。\n",
    "* 通过调用优化器来更新模型参数。\n",
    "\n",
    "为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。\n",
    "\n",
    "\n",
    "Implementing our model using the high-level APIs of a deep learning framework requires relatively less code. We don't need to allocate parameters individually, define our loss function, or manually implement mini-batch stochastic gradient descent. The advantages of high-level APIs become significantly greater when we need more complex models.\n",
    "\n",
    "Once we have all the basic components, [the training process code is very similar to what we did when implementing from scratch].\n",
    "\n",
    "To recap: in each iteration cycle, we will traverse the dataset (train_data) completely once, \n",
    "continuously obtaining a small batch of inputs and the corresponding labels. \n",
    "\n",
    "For each mini-batch, we will do the following steps:\n",
    "\n",
    "* Generate predictions by calling net(X) and calculate the loss l (forward propagation).\n",
    "* Compute gradients by performing backpropagation.\n",
    "* Update the model parameters by calling the optimizer.\n",
    "\n",
    "\n",
    "To better measure training effectiveness, we compute the loss after each iteration cycle and print it to monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363d1b84-f565-48d3-9926-c05a4cac76a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000271\n",
      "epoch 2, loss 0.000097\n",
      "epoch 3, loss 0.000097\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X) ,y) # net(X)自带模型参数所以我们不需要输入w b了 # 拿到预测值和真实的y做loss\n",
    "        trainer.zero_grad() # 清理gradients\n",
    "        l.backward() # Calculate the gradients using backward(); PyTorch automatically do sum for us.\n",
    "        trainer.step() # step()做模型更新\n",
    "    l = loss(net(features), labels) # 完成扫完一遍数据之后计算真实的loss；   features和labels的真实的损失\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76b6f8-317d-4eb7-af9a-5c4c0da49a9f",
   "metadata": {},
   "source": [
    "下面我们[**比较生成数据集的真实参数和通过有限数据训练获得的模型参数**]。\n",
    "要访问参数，我们首先从`net`访问所需的层，然后读取该层的权重和偏置。\n",
    "正如在从零开始实现中一样，我们估计得到的参数与生成数据的真实参数非常接近。\n",
    "\n",
    "\n",
    "Next, we [compare the real parameters of the dataset we generated with the model parameters obtained through training with limited data]. \n",
    "\n",
    "To access the parameters, we first access the required layer from net, and then read the weight and bias of that layer. \n",
    "\n",
    "Just as in the implementation from scratch, the parameters we estimated are very close to the real parameters that were used to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e2087d6-4450-4a2b-b54d-44798298e43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差 The estimated error of w： tensor([-0.0005,  0.0003])\n",
      "b的估计误差 The estimated error of b： tensor([-4.8637e-05])\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "print('w的估计误差 The estimated error of w：', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差 The estimated error of b：', true_b - b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
